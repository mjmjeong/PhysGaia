# ğŸŒ± PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis

This branch contains the implementation of [Shape-of-Motion (arxiv2407)](https://shape-of-motion.github.io/) using our PhysGaia dataset.
For general information about PhysGaia, please refer to the [main branch](https://github.com/mjmjeong/PhysGaia).

## **ğŸ’» Getting Started with Shape-of-Motion (SOM) on PhysGaia**


### Data Preprocessing

#### Installation

Setup preprocessing environment and download preprocessing model checkpoints:
```bash
cd preproc
./setup_dependencies.sh
```

#### Preprocessing Pipeline

Before training, preprocess the PhysGaia dataset to generate depth maps, object masks, and 2D tracks.

**1. Generate masks**
```bash
cd preproc

# Generate masks for FLIP dataset
CUDA_VISIBLE_DEVICES=0 python mask_app.py \
    --root_dir "FLIP/ship" \
    --img_name "render"

# Generate masks for Vellum dataset
CUDA_VISIBLE_DEVICES=0 python mask_app.py \
    --root_dir "/path/to/Vellum/flags" \
    --img_name "render"
```

**2. Process training data**
```bash
python process_physgaia.py \
    --img_dirs "/path/to/FLIP/ship/render/train" "/path/to/Vellum/flags/render/train" \
    --gpus 0 1 \
    --img_name "train" \
    --matching_pattern "0_*"
```

**3. Process test data**
```bash
python process_physgaia.py \
    --img_dirs "/path/to/FLIP/ship/render/test" "/path/to/Vellum/flags/render/test" \
    --gpus 0 1 \
    --img_name "test" \
```


#### Output Structure

After preprocessing, your directory structure will look like:
```
/path/to/FLIP/ship/
â”œâ”€â”€ render/
â”‚   â”œâ”€â”€ train/          # Original training images
â”‚   â””â”€â”€ test/           # Original test images
â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ train/          # Generated object masks
â”‚   â””â”€â”€ test/
â”œâ”€â”€ depth_anything_v2/
â”‚   â”œâ”€â”€ train/          # Raw depth maps
â”‚   â””â”€â”€ test/
â”œâ”€â”€ aligned_depth_anything_v2/
â”‚   â”œâ”€â”€ train/          # Aligned depth maps
â”‚   â””â”€â”€ test/
â”œâ”€â”€ bootstapir/
â”‚   â”œâ”€â”€ train/          # 2D tracks
â”‚   â””â”€â”€ test/
â”œâ”€â”€ droid_recon/
â”‚   â”œâ”€â”€ train/          # SLAM reconstruction
â”‚   â””â”€â”€ test/
â””â”€â”€ point_cloud.ply     # 3D point cloud
```


### Training
Train the model on a preprocessed PhysGaia scene:

```bash
python run_training.py \
  --work-dir ./outputs/ship_experiment \
  data:custom \
  --data.data-dir /path/to/FLIP/ship \
  --data.res train \
  --data.image-type render \
  --data.depth-type aligned_depth_anything_v2 \
  --data.camera-type camera_json \
  --data.camera-json-path /path/to/FLIP/ship/camera_info_train_mono.json \
  --data.track-2d-type bootstapir \
  --data.matching-pattern "0_*"
```

### Evaluation

```
python eval.py \
  --work-dir ./outputs/ship_experiment \
  --data-dir /path/to/FLIP/ship \
  --camera-json /path/to/FLIP/ship/camera_info_test.json \
  --output-dir ./outputs/ship_experiment/eval_test \
  --res test 
```

## **â­ï¸Â Key Highlights of PhysGaia**

- **ğŸ’¥Â Multi-body interaction**
- **ğŸ’Â Various materials**Â across all modalities
    - Liquid, Gas, Viscoelastic substance, and Textile
- **âœï¸Â Physical evaluation**
    - physics parameters
    - ground-truth 3D trajectories
- **ğŸ˜€Â Research friendly!!**
    - Providing codes for recent Dynamic Novel View Synthesis (DyNVS) models
        - [Shape-of-motion](https://github.com/mjmjeong/PhysGaia/tree/SOM), [4DGS](https://github.com/mjmjeong/PhysGaia/tree/4DGS_hex), [STG](https://github.com/mjmjeong/PhysGaia/tree/spacetime), [D-3DGS](https://github.com/mjmjeong/PhysGaia/tree/deformable)
    - Supporting **diverse training setting**: both monocular & multiview reconstruction

## **ğŸ”¥Â Ideal for â€œNextâ€ Research**

- ğŸ§  **Physical reasoning in dynamic scenes**
    - Offering ground-truth physics parameters for precise evaluation of inverse physics estimation
    - Offering ground-truth 3D trajectories for assessing actual motion beyond photorealism.
- ğŸ¤ **Multi-body physical interaction modeling**
- ğŸ§ª **Material-specific physics solver integration**
- ğŸ§¬ **Compatibility with existing DyNVS models**

## **ğŸ“‚ Dataset Structure**

Each folder is corresponding to each scene, containing the following files:

```bash
{material_type}_{scene_name}.zip
â”‚
â”œâ”€â”€ render/                              # Rendered images
â”‚   â”œâ”€â”€ train/                           # Images for training
â”‚   â””â”€â”€ test/                            # Images for evaluation
â”‚
â”œâ”€â”€ point_cloud.ply                      # COLMAP initialization (PatchMatch & downsampling)
â”œâ”€â”€ camera_info_test.json                # Monocular camera info for test
â”œâ”€â”€ camera_info_train_mono.json          # Monocular camera info for training
â”œâ”€â”€ camera_info_train_multi.json         # Multi-view camera info for training
â”‚
â”œâ”€â”€ {scene_name}.hipnc                   # Houdini source file (simulation or scene setup)
â”œâ”€â”€ particles/                           # Ground-truth trajectories

```

## **ğŸ‘©ğŸ»â€ğŸ’»Â Code implementation**

Please check each branch for integrated code for recent DyNVS methods.

- **Shape-of-motion (arXiv 24.07):** https://github.com/mjmjeong/PhysGaia/tree/SOM
- **4DGS (CVPR 24):** https://github.com/mjmjeong/PhysGaia/tree/4DGS_hex
- **STG (CVPR 24):** https://github.com/mjmjeong/PhysGaia/tree/spacetime
- **D-3DGS (CVPR 24):** https://github.com/mjmjeong/PhysGaia/tree/deformable

## **ğŸ’³ Citation**

```bash
TBD

```

## ğŸ¤ Contributing

We welcome contributions to expand the dataset (additional modality for new downstream tasks, , implementation for other models, etc.)

Reach out via opening an issue/discussion in the repo.

## ğŸ“¬ Contact

**Author**: Mijeong Kim & Gunhee Kim

ğŸ“§ Email: [mijeong.kim@snu.ac.kr](mailto:mijeong.kim@snu.ac.kr) & [gunhee2001@snu.ac.kr](mailto:gunhee2001@snu.ac.kr)

ğŸŒ LinkedIn: [Mijeong Kim](https://www.linkedin.com/in/mjmjeong) & [Gunhee Kim](https://www.linkedin.com/in/gunhee-kim-4072362b3/)

## ğŸ› ï¸ Future Plans

- Update fidelity of the generated scenes
- Add more easier scenes: providing more accessible starting points
- Add guidelines using Houdini source files: ex) How to obtain a flow field?

## **ğŸ’³ License**

This project is released under the Creative Commons Attribution-NonCommercial 4.0 license.

*âœ… Free to use, share, and adapt for non-commercial research*